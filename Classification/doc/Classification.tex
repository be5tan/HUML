%
%         File: Classification.tex
%  Description: Summary of elemetary results for classification.
%       Author: Bernhard Stankwitz <bernhard.stankewitz@posteo.de>
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[a4paper,11pt]{article} %{{{

%{{{ Packages
%===============================================================================
\usepackage[cm]{fullpage}                     % Small margins
\usepackage[english]{babel}
\usepackage[numbers]{natbib}                  % Bibtex
\usepackage{fontspec}                         % Xetex and bolder font
\usepackage{amssymb}                          % Mathbb
\usepackage{amsthm}	                          % Theoremstyle proof environmen
\usepackage{amsmath}
\usepackage{upgreek}                          % Second upper case Theta
\usepackage{mathrsfs}
\usepackage{graphicx}                         % Figures
\usepackage{float}                            % Figures
\usepackage{enumitem}
\usepackage{xcolor}                           % Colors 
\usepackage{sectsty}                          % Individually colored sections
\usepackage{hyperref}
%}}}

%{{{ Color options
%===============================================================================
\definecolor{gruvbg}{HTML}{282828}
\definecolor{gruvfg}{HTML}{ebdbb2}
\definecolor{gruvyellow}{HTML}{d79921}
\definecolor{gruvred}{HTML}{cc241d}
\definecolor{gruvblue}{HTML}{458588}
\definecolor{gruvgreen}{HTML}{98971a}

\pagecolor{gruvbg}                            % background color
\color{gruvfg}                                % foreground color
\sectionfont{\color{gruvyellow}}              % Sets color of sections
\subsectionfont{\color{gruvyellow}}           % Sets color of subsections
\hypersetup{
    linktocpage = true,
    colorlinks = true,                        % Colors the border
    linkcolor = gruvred,
    citecolor = gruvgreen,
    urlcolor = gruvblue
}

\numberwithin{equation}{section}
% \setcounter{section}{+1}                    % Contral Chapter number
%}}}

%{{{ Math operators
%===============================================================================
\DeclareMathOperator*{\argmin}{arg\!\min}
\DeclareMathOperator*{\argmax}{arg\!\max}
\DeclareMathOperator*{\vspan}{span}
\DeclareMathOperator*{\tr}{tr}
\DeclareMathOperator*{\im}{im}
\DeclareMathOperator*{\rk}{rk}
\DeclareMathOperator*{\kl}{KL}
\DeclareMathOperator*{\Bin}{Bin}
\DeclareMathOperator*{\Poi}{Poi}
\DeclareMathOperator*{\CPoi}{CPoi}
\DeclareMathOperator*{\Unif}{Unif}
\DeclareMathOperator*{\Exp}{Exp}
\DeclareMathOperator*{\sgn}{sgn}
\DeclareMathOperator*{\co}{co}
\DeclareMathOperator*{\inn}{int}
\DeclareMathOperator*{\dom}{dom}
\DeclareMathOperator*{\epi}{epi}
\DeclareMathOperator*{\conv}{Conv}
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}
\DeclareMathOperator*{\asin}{asin}
\DeclareMathOperator*{\acos}{acos}
\DeclareMathOperator*{\atan}{atan}
\DeclareMathOperator*{\acot}{acot}
%}}}

%{{{ Theoremstyles
%===============================================================================
\newtheoremstyle{ndefinition}  % Name 
  {\topsep}                    % Space above theorem, e.g. 3pt
  {\topsep}                    % Space below theorem, e.g. 3pt
  {\color{gruvblue}}           % Font in body of theorem
  {0pt}                        % Space to indent
  {\bfseries\color{gruvblue}}  % Name of head font
  {.}                          % Punctuation between head and body
  { }                          % Space after theorem head; " " = normal interword space
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
\newtheoremstyle{nplain}
  {\topsep}
  {\topsep}
  {\itshape\color{gruvblue}}
  {0pt}
  {\bfseries\color{gruvblue}}
  {.}
  { }
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
\newtheoremstyle{nremark}
  {\topsep}
  {\topsep}
  {}
  {0pt}
  {\itshape\color{gruvblue}}
  {.}
  { }
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
\renewenvironment{proof}
  {{\noindent \itshape \color{gruvblue} Proof. }}{\color{gruvblue}\qed}

\theoremstyle{ndefinition}
\newtheorem{defi}{Definition}[section]
\newtheorem{exer}[defi]{Exercise}
\newtheorem{aufg}[defi]{Aufgabe}
\newtheorem{alg}[defi]{Algorithm}
  
\theoremstyle{nremark}
\newtheorem{expl}[defi]{Example}
\newtheorem{bsp}[defi]{Beispiel}
\newtheorem{bem}[defi]{Bemerkung}
\newtheorem{rem}[defi]{Remark}

\theoremstyle{nplain}
\newtheorem{lem}[defi]{Lemma}
\newtheorem{stz}[defi]{Satz}
\newtheorem{cor}[defi]{Corollary}
\newtheorem{thm}[defi]{Theorem}
\newtheorem{prp}[defi]{Proposition}
%}}}

%}}}

\begin{document}
\title{Classification}
\author{Bernhard Stankewitz \\ \texttt{bernhard.stankewitz@posteo.de}}
\maketitle
\abstract{Summary of elemetary results for classification.}
\tableofcontents

\section{Elementary definitions and the Bayes classifier}%{{{
\label{sec:ElementaryDefinitionsAndTheBayesClassifier}

\begin{defi}[Classifier]%{{{
  \label{def:Classifier}
  \
  \begin{enumerate}[label=(\alph*)]
    \item For i.i.d. training data \( ( X_{1}, Y_{1} ), \dots, ( X_{n}, Y_{n} )
      \in \mathbb{R}^{d} \times \{ 1, \dots, K \} \), a classifier is a
      measurable function \( C : \mathbb{R}^{d} \to \{ 1, \dots, K \} \). The
      classification error is given by
      \begin{align*}
        R(C) 
        & : = \mathbb{P} \{ C(X) \ne Y \} 
          =   \mathbb{E} \mathbf{1}_{C(X) \ne Y}. 
      \end{align*}
    \item In case that the labels are given by \( \{ 0, 1 \} \), the
      classification error 
      \begin{align*}
        R(C) 
        & = \mathbb{E} ( Y - C(X) )^{2} 
        = \int \mathbf{1}_{y \ne C(x)} \, \mathbb{P}^{X, Y}(d(x, y)). 
      \end{align*}
  \end{enumerate}
\end{defi}%}}}

In case all theoretical quantities are known, a classification problem has an
optimal solution.
\begin{prp}[Bayes-Classifier]%{{{
  \label{prp:BayesClassifier}
  \
  \begin{enumerate}[label=(\roman*)]
    \item In the situation of Definition \ref{def:Classifier}, the
      classification error is minimised by the Bayes classifier
      \begin{align*}
        C^{\text{Bayes}}(x) 
        & : = \argmax_{k = 1, \dots, K} \mathbb{P} \{ Y = k | X = x \}. 
      \end{align*}
    \item If the labels are given by \( \{ 0, 1 \} \), we have
      \begin{align*}
        C^{\text{Bayes}}(x) 
        & = \mathbf{1}_{} \{ \eta(x) \ge 1 / 2 \} 
        \qquad \text{ with } \qquad 
        \eta(x) : = \mathbb{P} \{ Y = 1 | X = x \}.
      \end{align*}
  \end{enumerate}
\end{prp}%}}}
\begin{proof}%{{{
  For any classifier \( C \), we have
  \begin{align*}
    R(C) 
    & =   1 - \mathbb{E} \mathbb{E} \mathbf{1}_{C = Y} | X 
      =   1 
          - \mathbb{E} \sum_{k = 1}^{K} 
            \mathbb{E} ( \mathbf{1}_{C = k} \mathbf{1}_{Y = k} | X ) 
      =   1 
          - \mathbb{E} \sum_{k = 1}^{K} 
            \mathbf{1}_{C = k}  
            \mathbb{E} ( \mathbf{1}_{Y = k} | X ). 
  \end{align*}
\end{proof}%}}}

%}}} section Elementary definitions and the Bayes classifier (end) 

\section{The KNN-classifier}%{{{
\label{sec:TheKNNClassifier}

\begin{defi}[KNN-classifier]%{{{
  \label{def:KNNClassifier}
  \
  \begin{enumerate}[label=(\alph*)]
    \item Let \( ( X_{1}, Y_{1} ), \dots, ( X_{n}, Y_{n} ) \in \mathbb{R}^{d}
      \times \{ 1, \dots, J \} \) be a training sample and \( K \in \mathbb{N}
      \). For \( x \in \mathbb{R}^{d} \), let \( N_{K}(x) \) be the set of the
      \( K \) nearest neighbours of \( x \) with respect to the euclidean
      distance. Then, the KNN-classifier is given by
      \begin{align*}
        \hat C^{\text{KNN}}(x) 
        & : = \argmax_{j = 1, \dots, J} 
        \frac{1}{K} \sum_{X_{i} \in N_{K}(x)} \mathbf{1}_{Y_{i} = j}.  
      \end{align*}
    \item In case, the labels are given by \( \{ 0, 1 \} \), we have
      \begin{align*}
        \hat C^{\text{KNN}}(x) 
        = \mathbf{1}_{} \{ \hat \eta(x) \ge 1 / 2 \} 
        \qquad \text{ with } \qquad 
        \hat \eta(x) 
        : = \frac{1}{K} \sum_{X_{i} \in N_{K}(x)} \mathbf{1}_{Y_{i} = 1} 
        = : \sum_{i = 1}^{n} w_{i}(x) Y_{i},
      \end{align*}
      where \( w_{i} : = \mathbf{1}_{X_{i} \in N_{K}(x)} / K  \) with \( \sum_{i
      = 1}^{n} w_{i} = 1 \). 
  \end{enumerate}
\end{defi}%}}}

\begin{lem}[Reduction to the regression function]%{{{
  \label{lem:ReductionToTheRegressionFunction}
  In the situation of Definition \ref{def:KNNClassifier} (b), we have that 
  \begin{align*}
    | \mathbb{E}_{\le n} R(\hat C^{\text{KNN}}) - R(C^{\text{Bayes}}) | 
    & \le \sqrt{\mathbb{E}_{n + 1} | \hat \eta(X) - \eta(X) |^{2}} 
  \end{align*}
\end{lem}%}}}
\begin{proof}%{{{
  For any classifier \( C \), we have 
  \begin{align*}
    \mathbb{P} \{ C(X) = Y | X \} 
    & = \mathbf{1}_{C = 1} \eta + \mathbf{1}_{C = 0} ( 1 - \eta ) 
      = \eta + \mathbf{1}_{C = 0} ( 1 - 2 \eta ). 
  \end{align*}
  This yields 
  \begin{align*}
    | 
        \mathbb{P} \{ \hat C^{\text{KNN}}(X) \ne Y | X \} 
      & 
      - \mathbb{P} \{ \hat C^{\text{Bayes}}(X) \ne Y | X \} 
    | 
      = | 
            \mathbb{P} \{ \hat C^{\text{KNN}}(X) = Y | X \} 
          - \mathbb{P} \{ \hat C^{\text{Bayes}}(X) = Y | X \} 
        | 
    \\ 
    & = | 
            \hat \eta + \mathbf{1}_{\hat C^{\text{KNN}} = 0} ( 1 - 2 \hat \eta ) 
          - \eta + \mathbf{1}_{C^{\text{Bayes}} = 0} ( 1 - 2 \eta )
        |
    \\ 
    & = \begin{cases}
          | \hat \eta - \eta |, & \hat C^{\text{KNN}} = C^{\text{Bayes}}, \\ 
          | 1 - \hat \eta - \eta |, & \hat C^{\text{KNN}} = C^{\text{Bayes}}.
        \end{cases}
    \\ 
    & \le \begin{cases}
            | \hat \eta - \eta |, & \hat C^{\text{KNN}} = C^{\text{Bayes}}, \\ 
            | 1 / 2 - \hat \eta \land  \eta |,
            & \hat C^{\text{KNN}} = C^{\text{Bayes}}.
          \end{cases}
      \le | \hat \eta - \eta |.
  \end{align*}
  For the second to last step, use that one of \( \eta \) and \( \hat \eta \)
  has to be above and one below \( 1 / 2 \). 
  By conditioning on \( X \) and Jensen's inequality, this gives
  \begin{align*}
    | \mathbb{E}_{\le n} R(\hat C^{\text{KNN}}) - R(C^{\text{Bayes}}) |^{2} 
    & = | 
          \mathbb{E}_{\le n + 1} ( 
              \mathbf{1}_{\hat C^{\text{KNN}} \ne Y} 
            - \mathbf{1}_{C^{\text{Bayes}} \ne Y}
          )
        |^{2} 
      \le \mathbb{E}_{\le n + 1} | \hat \eta - \eta |^{2}.
  \end{align*}
\end{proof}%}}}

\begin{thm}[Consistency of KNN]%{{{
  \label{thm:ConsistencyOfKNN}
  In the situation of Definition \ref{def:KNNClassifier} (b), let \( k \to
  \infty \), \( k / n \to 0 \) and let \( x \mapsto \eta(x) \) be uniformly
  continuous. Then, the KNN-classifier \( \hat C^{\text{KNN}} \) is consistent,
  i.e. 
  \begin{align*}
    | \mathbb{E}_{\le n} R(\hat C^{\text{KNN}}) - R(C^{\text{Bayes}}) | 
    & \xrightarrow[]{n \to \infty} 0.
  \end{align*}
\end{thm}%}}}
\begin{proof}%{{{
  From Lemma \ref{lem:ReductionToTheRegressionFunction}, we obtain with the
  triangle inequality
  \begin{align*}
    | \mathbb{E}_{\le n} R(\hat C^{\text{KNN}}) - R(C^{\text{Bayes}}) | 
    & \le \sqrt{\mathbb{E}_{\le n + 1} | \hat \eta(X) - \eta(X) |^{2}} 
      = \sqrt{
          \mathbb{E}_{\le n + 1} | 
            \sum_{i = 1}^{n} w_{i}(X) ( Y_{i} - \eta(X) )
          |^{2}
        } 
    \\ 
    & \le   \sqrt{
              \mathbb{E}_{\le n + 1} | 
                \sum_{i = 1}^{n} w_{i}(X) ( Y_{i} - \eta(X_{i}) )
              |^{2}
            } 
          + \sqrt{
              \mathbb{E}_{\le n + 1} | 
                \sum_{i = 1}^{n} w_{i}(X) ( \eta(X_{i}) - \eta(X) )
              |^{2}
            } 
  \end{align*}
  In the following, we consider the two terms separately.

  For the first term, we have by independence 
  \begin{align*}
    \mathbb{E}_{\le n + 1} | 
      \sum_{i = 1}^{n} w_{i}(X) ( Y_{i} - \eta(X_{i}) )
    |^{2} 
    & = \mathbb{E}_{\le n + 1}
        \sum_{i = 1}^{n} 
        w_{i}(X)^{2} ( Y_{i} - \eta(X_{i}) )^{2} 
    \\ 
    & \le \mathbb{E}_{\le n + 1} \Big( 
            \max_{i \le n} w_{i}(X) 
            \sum_{i = 1}^{n} w_{i}(X)
          \Big) 
      \le 1 / K 
      \xrightarrow[]{n \to \infty} 0. 
  \end{align*}

  For the second term, we have for any \( \varepsilon > 0 \) and corresponding
  \( \delta > 0 \), that 
  \begin{align*}
    \mathbb{E}_{\le n + 1} | 
      \sum_{i = 1}^{n} w_{i}(X) ( \eta(X_{i}) - \eta(X) )^{2} 
    | 
    & \le \mathbb{E}_{\le n + 1} | 
            \sum_{i = 1}^{n} w_{i}(X) \mathbf{1}_{| X_{i} - X | \ge \delta} 
            + \varepsilon
          | 
    \\ 
    & \le 
  \end{align*}
  <++>
  
\end{proof}%}}}

%}}} section The KNN-classifier (end) 

% \bibliographystyle{plainnat}
% \bibliography{references.bib}

\end{document}
